{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정리\n",
    "\n",
    "## MDP\n",
    "---\n",
    "\n",
    "\n",
    "순차적 행동결정 문제를 수학적으로 정의한 것, **MDP**는 다음과 같은 항목으로 구성하고 있다.\n",
    "\n",
    "- $S$ (state) : \n",
    "- $A$ (action) :\n",
    "- $R$ (reward) :\n",
    "- $P_{ss'}^{a}$ (state transition probability) :\n",
    "- $\\gamma$ (discount factor) : \n",
    "- $\\pi(a,s)$ (policy): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value function\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent가 어떤 $\\pi$(정책)가 더 좋은 정책인지 판단하는 기준이 value function\n",
    "#### **s**(현재 상태)로부터 $\\pi$(정책)을 따라갔을 때 받을 것이라 예상되는 Reward의 합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **state-value function**\n",
    ">\n",
    "$$\n",
    "\\mathbf{v}_\\pi(\\mathbf{s}) = \\mathbb{E}_\\pi[\\mathbf{R_{t+1}} + \\gamma \\mathbf{v}_\\pi(\\mathbf{S}_{t+1}) \\ | \\ \\mathbf{S}_{t} = \\mathbf{s}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Agent는 $\\pi$(정책)을 업데이트할 때 value function를 사용할 텐데, 보통 value function보다는 value function가 선택할 각 action의 value를 직접적으로 나타내는 q function을 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **action-value function**\n",
    ">\n",
    "$$\n",
    "\\mathbf{q}_\\pi(\\mathbf{s, a}) = \\mathbb{E}_\\pi[\\mathbf{R_{t+1}} + \\gamma \\mathbf{q}_\\pi(\\mathbf{S}_{t+1}, \\mathbf{A}_{t+1}) \\ | \\ \\mathbf{S}_{t} = \\mathbf{s}, \\mathbf{A}_{t} = \\mathbf{a}]\n",
    "$$\n",
    ">\n",
    "> **action-value function** (연산가능하도록 또 다른 형태)\n",
    ">\n",
    "$$\n",
    "\\mathbf{q}_\\pi(\\mathbf{s, a}) = \\mathbf{R_{t+1}} + \\gamma \\sum_{\\mathbf{s'} \\in S} \\mathbf{P_{ss'}^{\\mathbf{a}}} \\mathbf{v}_\\pi(\\mathbf{s'})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman equation\n",
    "---\n",
    "\n",
    "> $\\mathbf{s}_t$(현재 상태)의 가치함수와 $\\mathbf{s}_{t+1}$(다음 상태)의 가치함수 관계식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Bellman Expectation Equation** : \n",
    "> 특정 정책을 따라갔을 때 **state-value function** 사이의 관계식\n",
    ">\n",
    "$$\n",
    "\\mathbf{v}_\\pi(\\mathbf{s}) = \\mathbb{E}_\\pi[\\mathbf{R_{t+1}} + \\gamma \\mathbf{v}_\\pi(\\mathbf{S}_{t+1}) \\ | \\ \\mathbf{S}_{t} = \\mathbf{s}]\n",
    "$$\n",
    "> 가치 함수와 큐 함수의 관계식 (계산 O)\n",
    "> \n",
    "$$\n",
    "\\mathbf{v}_\\pi(\\mathbf{s}) = \\sum_{\\mathbf{a} \\in A} \\pi(\\mathbf{a}|\\mathbf{s}) \\ \\mathbf{q}_\\pi(\\mathbf{s, a})\n",
    "$$\n",
    ">\n",
    "> 계산 가능한 Bellman Equation\n",
    ">\n",
    "$$\n",
    "\\mathbf{v}_\\pi(\\mathbf{s}) = \\sum_{\\mathbf{a} \\in A} \\pi(\\mathbf{a}|\\mathbf{s}) \\ (\\mathbf{R_{t+1}} + \\gamma \\sum_{\\mathbf{s'} \\in S} \\mathbf{P_{ss'}^{\\mathbf{a}}} \\mathbf{v}_\\pi(\\mathbf{s'}))\n",
    "$$\n",
    ">\n",
    "> 계산 가능한 Bellman Equation\n",
    ">\n",
    "$$\n",
    "\\mathbf{v}_\\pi(\\mathbf{s}) = \\sum_{\\mathbf{a} \\in A} \\pi(\\mathbf{a}|\\mathbf{s}) \\ (\\mathbf{R_{t+1}} + \\gamma \\sum_{\\mathbf{s'} \\in S} \\mathbf{P_{ss'}^{\\mathbf{a}}} \\mathbf{v}_\\pi(\\mathbf{s'}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Bellman Optimality Equation**\n",
    "> 최적의 가치함수를 받게하는 정책을 따라갔을 때 가치함수 사이의 관계식\n",
    ">\n",
    "\n",
    "$$\n",
    "\\mathbf{v}_*(\\mathbf{s}) = \\max_{a} \\ \\mathbb{E}[\\mathbf{R_{t+1}} + \\gamma \\mathbf{v}_*(\\mathbf{S}_{t+1}) \\ | \\ \\mathbf{S}_{t} = \\mathbf{s}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Programming\n",
    "___\n",
    "\n",
    "- **Dynamic** : sequential or temporal component to the problem\n",
    "- **Programming** : optimising a “program”, i.e. a policy\n",
    "    - A method for solving complex problems\n",
    "    - By breaking them down into subproblems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bellman Expectation Equation을 푸는 것이 **Policy Iteration**  \n",
    "- Bellman Optimality Equation을 푸는 것이 **Value Iteration** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Policy Iteration\n",
    "\n",
    "Bellman Expectation Equation 푸는 것 : 주변 상태의 value function과 다음 스텝의 보상만 고려하여 현재 상태의 다음 가치함수를 계산\n",
    "\n",
    "Policy Iteration는 Policy evaluation 과 Policy improvement로 구성\n",
    "\n",
    "- Policy evaluation : Estimate $\\mathbf{v}_\\pi$ Iterative policy evaluation\n",
    "- Policy improvement : Generate $\\pi^{'}$ $\\ge$ $\\pi$ Greedy policy improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/DeepHaeJoong/RL/blob/master/RL_Document/FIGURE/FIG_1.png?raw=true\\\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Policy evaluation (정책 평가)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Bellman Expectation Equation**을 통한 효율적인 **value function** 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{v}_\\pi(\\mathbf{s}) = \\mathbb{E}_\\pi[\\mathbf{R_{t+1}} + \\gamma \\mathbf{v}_\\pi(\\mathbf{S}_{t+1}) \\ | \\ \\mathbf{S}_{t} = \\mathbf{s}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 합의 형태로 표현한 **Bellman Equation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{v}_\\pi(\\mathbf{s}) = \\sum_{\\mathbf{a} \\in A} \\pi(\\mathbf{a}|\\mathbf{s}) \\ (\\mathbf{R_{t+1}} + \\gamma \\sum_{\\mathbf{s'} \\in S} \\mathbf{P_{ss'}^{\\mathbf{a}}} \\mathbf{v}_\\pi(\\mathbf{s'}))\n",
    "$$\n",
    "\n",
    "> $\\pi$라는 정책에 대해 반복적으로 수행 (k = 1, 2, 3, 4, ....)\n",
    "\n",
    "$$\n",
    "\\mathbf{v}_{k+1}(\\mathbf{s}) = \\sum_{\\mathbf{a} \\in A} \\pi(\\mathbf{a}|\\mathbf{s}) \\ (\\mathbf{R_{t+1}} + \\gamma \\sum_{\\mathbf{s'} \\in S} \\mathbf{P_{ss'}^{\\mathbf{a}}} \\mathbf{v}_{k}(\\mathbf{s'}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example :  grid world\n",
    "\n",
    "- $S$ (state) : red box\n",
    "- $A$ (action) : [up, down, left, right]\n",
    "- $R$ (reward) : Green tringle (-1), Blue circle (+1)\n",
    "- $P_{ss'}^{\\mathbf{a}}$ (state transition probability) : 1\n",
    "- $\\gamma$ (discount factor) : 0.9\n",
    "- $\\pi(\\mathbf{a} | \\mathbf{s})$ (policy): $\\frac{1}{4}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/DeepHaeJoong/RL/blob/master/RL_Document/FIGURE/FIG_2.png?raw=true\\\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 알고리즘\n",
    "\n",
    "**1.** k번째 **value function** Matrix에서 $\\mathbf{s}$에서 갈 수 있는 다음 상태 $\\mathbf{s'}$에 저장돼 있는 value function $\\mathbf{v}_k (\\mathbf{s'})$을 불러온다.\n",
    "\n",
    "\n",
    "**2.** $\\mathbf{v}_k (\\mathbf{s'})$에 $\\gamma$를 곱하고 그 상태로 가능 행동에 대한 보상($\\mathbf{R_{s}^{\\mathbf{a}}}$)을 더한다.\n",
    "\n",
    "$$\n",
    "\\mathbf{R_{s}^{\\mathbf{a}}} + \\gamma \\mathbf{v}_k (\\mathbf{s'})\n",
    "$$\n",
    "\n",
    "\n",
    "**3.** 2번에서 구한 값에 그 행동을 할 확률, 즉 정책을 곱한다.\n",
    "\n",
    "$$\n",
    "\\pi(\\mathbf{a}|\\mathbf{s})(\\mathbf{R_{s}^{\\mathbf{a}}} + \\gamma \\mathbf{v}_k (\\mathbf{s'}))\n",
    "$$\n",
    "\n",
    "\n",
    "**4.** 3번을 모든 선택 가능한 행동에 대해 반복하고 그 값들을 더한다.\n",
    "\n",
    "$$\n",
    "\\sum_{\\mathbf{a} \\in A} \\pi(\\mathbf{a}|\\mathbf{s})(\\mathbf{R_{s}^{\\mathbf{a}}} + \\gamma \\mathbf{v}_k (\\mathbf{s'}))\n",
    "$$\n",
    "\n",
    "\n",
    "**5.** 4번 과정을 통해 더한 값을 k+1 번째 가치함수 행렬에 상태 s자리에 저장한다.\n",
    "\n",
    "\n",
    "**6.** 1-5 과정이 모든 $\\mathbf{s} \\in S$에 대해 반복한다.\n",
    "\n",
    "$$\n",
    "\\mathbf{s} \\in S\n",
    "$$\n",
    "\n",
    "\n",
    "$\\pi$라는 정책에 대해 반복적으로 수행 (k = 1, 2, 3, 4, ....)하면 $\\mathbf{v}_\\pi$가 수렴될 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Policy improvement (정책 발전)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Greedy Policy Improvement** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 앞서 정책 평가를 통해 구한 것은 에이전트가 정책을 따랐을 때의 모든 상태에 대한 가치 함수(**state-value function**)이다. 어떻게 이 가치함수를 통해 각 상태에 대해 어떤 행동을 하는 것이 좋은지 알 수 있을까? 큐함수(**action-value function**)을 이용하면 어떤 행동이 좋은지 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **action-value function(큐함수)** 정의 \n",
    "> \n",
    "$$\n",
    "\\mathbf{q}_\\pi(\\mathbf{s, a}) = \\mathbb{E}_\\pi[\\mathbf{R_{t+1}} + \\gamma \\mathbf{q}_\\pi(\\mathbf{S}_{t+1}, \\mathbf{A}_{t+1}) \\ | \\ \\mathbf{S}_{t} = \\mathbf{s}, \\mathbf{A}_{t} = \\mathbf{a}]\n",
    "$$\n",
    "\n",
    "> **action-value function** (연산가능하도록 또 다른 형태)\n",
    ">\n",
    "$$\n",
    "\\mathbf{q}_\\pi(\\mathbf{s, a}) = \\mathbf{R_{t+1}} + \\gamma \\sum_{\\mathbf{s'} \\in S} \\mathbf{P_{ss'}^{\\mathbf{a}}} \\mathbf{v}_\\pi(\\mathbf{s'})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent가 해야할 일은 상태 $\\mathbf{s}$에서 선택 가능한 행동 $\\mathbf{q}_\\pi(\\mathbf{s, a})$를 비교하고 그중에서 가장 큰 큐함수를 가지는 행동을 선택하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Greedy Policy Improvement**으로 얻은 새로운 정책\n",
    ">\n",
    "$$\n",
    "\\pi^{'}(\\mathbf{s}) = \\mathbf{argmax}_{\\mathbf{a} \\in A} \\mathbf{q}_\\pi(\\mathbf{s, a})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/DeepHaeJoong/RL/blob/master/RL_Document/FIGURE/FIG_3.png?raw=true\\\" alt=\"drawing\" width=\"700\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
