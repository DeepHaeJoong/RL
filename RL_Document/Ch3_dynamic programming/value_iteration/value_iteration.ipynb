{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Programming\n",
    "___\n",
    "\n",
    "- **Dynamic** : sequential or temporal component to the problem\n",
    "- **Programming** : optimising a “program”, i.e. a policy\n",
    "    - A method for solving complex problems\n",
    "    - By breaking them down into subproblems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bellman Expectation Equation을 푸는 것이 **Policy Iteration** \n",
    "- Bellman Optimality Equation을 푸는 것이 **Value Iteration** (지금 다루고자 하는 내용) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Value Iteration\n",
    "\n",
    "**Bellman Optimality Equation**을 통해 구함 : 그저 현재 상태에서 가능한 **$\\mathbf{R_{t+1}} + \\gamma \\mathbf{v}_k (\\mathbf{S}_{t+1})$**의 값들 중에서 최고의 값으로 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Bellman Optimality Equation**을 통해 구하는 것은 최적 가치함수이다.\n",
    "> \n",
    "$$\n",
    "\\mathbf{v}_*(\\mathbf{s}) = \\max_{\\mathbf{a}} \\ \\mathbb{E}[\\mathbf{R_{t+1}} + \\gamma \\mathbf{v}_*(\\mathbf{S}_{t+1}) \\ | \\ \\mathbf{S}_{t} = \\mathbf{s}, \\mathbf{A}_{t} = \\mathbf{a}]\n",
    "$$\n",
    "\n",
    "> **Bellman Optimality Equation** (계산 가능)\n",
    "> \n",
    "$$\n",
    "\\mathbf{v}_{k+1}(\\mathbf{s}) = \\max_{\\mathbf{a}} (\\mathbf{R_{t+1}} + \\gamma \\mathbf{v}_{k}(\\mathbf{s'}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**그림으로 이해하기** (value function 계산하는 방법)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/DeepHaeJoong/RL/blob/master/RL_Document/Ch3_dynamic%20programming/value_iteration/FIGURE/FIG_1.png?raw=true\" alt=\"drawing\" width=\"700\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Code \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from environment import GraphicDisplay, Env\n",
    "\n",
    "class ValueIteration:\n",
    "    def __init__(self, env):\n",
    "        # 환경 객체 생성\n",
    "        self.env = env\n",
    "        # 가치 함수를 2차원 리스트로 초기화\n",
    "        self.value_table = [[0.0] * env.width for _ in range(env.height)]\n",
    "        # 감가율\n",
    "        self.discount_factor = 0.9\n",
    "\n",
    "    # 가치 이터레이션\n",
    "    # 벨만 최적 방정식을 통해 다음 가치 함수 계산\n",
    "    def value_iteration(self):\n",
    "        next_value_table = [[0.0] * self.env.width for _ in\n",
    "                            range(self.env.height)]\n",
    "        for state in self.env.get_all_states():\n",
    "            if state == [2, 2]:\n",
    "                next_value_table[state[0]][state[1]] = 0.0\n",
    "                continue\n",
    "            # 가치 함수를 위한 빈 리스트\n",
    "            value_list = []\n",
    "\n",
    "            # 가능한 모든 행동에 대해 계산\n",
    "            for action in self.env.possible_actions:\n",
    "                next_state = self.env.state_after_action(state, action)\n",
    "                reward = self.env.get_reward(state, action)\n",
    "                next_value = self.get_value(next_state)\n",
    "                value_list.append((reward + self.discount_factor * next_value))\n",
    "            # 최댓값을 다음 가치 함수로 대입\n",
    "            next_value_table[state[0]][state[1]] = round(max(value_list), 2)\n",
    "        self.value_table = next_value_table\n",
    "\n",
    "    # 현재 가치 함수로부터 행동을 반환\n",
    "    def get_action(self, state):\n",
    "        action_list = []\n",
    "        max_value = -99999\n",
    "\n",
    "        if state == [2, 2]:\n",
    "            return []\n",
    "\n",
    "        # 모든 행동에 대해 큐함수 (보상 + (감가율 * 다음 상태 가치함수))를 계산\n",
    "        # 최대 큐 함수를 가진 행동(복수일 경우 여러 개)을 반환\n",
    "        for action in self.env.possible_actions:\n",
    "\n",
    "            next_state = self.env.state_after_action(state, action)\n",
    "            reward = self.env.get_reward(state, action)\n",
    "            next_value = self.get_value(next_state)\n",
    "            value = (reward + self.discount_factor * next_value)\n",
    "\n",
    "            if value > max_value:\n",
    "                action_list.clear()\n",
    "                action_list.append(action)\n",
    "                max_value = value\n",
    "            elif value == max_value:\n",
    "                action_list.append(action)\n",
    "\n",
    "        return action_list\n",
    "\n",
    "    def get_value(self, state):\n",
    "        return round(self.value_table[state[0]][state[1]], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dynamic Programming의 한계와 강화학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습 X, 머신러닝이 아님\n",
    "\n",
    "**크게 3가지**\n",
    "\n",
    "- 계산 복잡도\n",
    "- 차원의 저주\n",
    "- 환경에 대한 완벽한 정보 필요\n",
    " \n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
