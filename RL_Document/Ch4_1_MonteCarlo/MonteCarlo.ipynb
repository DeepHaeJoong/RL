{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-Carlo Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Dynamic Programming`과 `Reinforcement Learning`의 차이 : RL은 환경의 모델을 몰라도 환경과의 상호작용을 통해 최적 정책을 학습하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction**\n",
    "\n",
    "가치함수를 학습하는 것, 크게 두 가지 방법이 존재\n",
    "\n",
    "- **MonteCarlo** \n",
    "- Temporal-difference (TD)\n",
    "\n",
    "**Control**\n",
    "\n",
    "가치함수를 토대로 정책을 발전시켜 최적 정책을 찾는 것\n",
    "\n",
    "- SARSA\n",
    "- Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 강화학습과 정책 평가 1 : Monte-Carlo Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화학습 과정\n",
    "\n",
    "1. 일단 진행\n",
    "2. 자신 평가\n",
    "3. 평가\n",
    "4. 업데이트\n",
    "\n",
    "5. 1-4 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 강화학습의 예측과 제어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`'Dynamic Programming'`에서의 Policy Iteration 방법 중 `Policy evaluation`은  `bellman expectation equation`을 사용하여 `value function`을 계산하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **bellman expectation equation**\n",
    "> \n",
    "$$\n",
    "\\mathbf{v}_\\pi(\\mathbf{s}) = \\mathbb{E}_\\pi[\\mathbf{R_{t+1}} + \\gamma \\mathbf{v}_\\pi(\\mathbf{S}_{t+1}) \\ | \\ \\mathbf{S}_{t} = \\mathbf{s}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만, 강화학습에서는 위와 같이 **계산**을 통해서 **`value function`**을 알아내는 것이 아니라 **Agent**가 겪은 경험으로부터 예측하여 **`value function`**(참 값)을 업데이트 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MC 근사의 예시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보통 원의 넓이를 계산하기 위해 원의 방정식을 이용한다.\n",
    "\n",
    "> 원의 넓이를 구하는 방정식\n",
    "$$\n",
    "\\mathbf{S} = \\pi\\mathbf{r}^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1)** 하지만, 방정식을 모른다면 어떻게 계산할 수 있을까?\n",
    "\n",
    "**A1)** **Monte-Carlo Approximation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Monte-Carlo Approximation` : \"무작위로 무엇인가를 해본다\", \"Sample\"을 통해 \"원래의 값에 대해 이럴 것이다\"라고 추정하는 것, 무작위로 많이하다보면 원래의 값과 비슷해짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example1)** \n",
    "종이 위에 그려진 원의 넓이를 구하려면 어떻게 해야 할까?\n",
    "\n",
    "$$\n",
    "\\frac{\\mathbf{S(A)}}{\\mathbf{S(B)}} \\sim \\frac{\\mathbf{1}}{\\mathbf{11}} \\sum_{i = 1}^{11} \\mathbf{I(\\text{red_dot}_i \\in \\mathbf{A})} = \\frac{\\mathbf{4}}{\\mathbf{11}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MC 추정은 무한히 반복하면 원래 값과 동일해진다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "$\n",
    "\\text{if n } \\to \\infty , \\text{then} \\\\\n",
    "\\frac{1}{n} \\sum_{i = 1}^{n} \\mathbf{I(\\text{red_dot}_i \\in \\mathbf{A})} = \\frac{\\mathbf{S(A)}}{\\mathbf{S(B)}}\n",
    "$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "장점 : 근사의 장점은 방정식을 몰라도 원래 값을 추정할 수 있는 점,\n",
    "\n",
    "강화학습에서 MC 근사를 어떻게 사용하는지 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 샘플링과 몬테카를로 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
