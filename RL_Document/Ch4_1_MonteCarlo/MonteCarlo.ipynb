{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-Carlo Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Dynamic Programming`과 `Reinforcement Learning`의 차이 : RL은 환경의 모델을 몰라도 환경과의 상호작용을 통해 최적 정책을 학습하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction**\n",
    "\n",
    "가치함수를 학습하는 것, 크게 두 가지 방법이 존재\n",
    "\n",
    "- **MonteCarlo** \n",
    "- Temporal-difference (TD)\n",
    "\n",
    "**Control**\n",
    "\n",
    "가치함수를 토대로 정책을 발전시켜 최적 정책을 찾는 것\n",
    "\n",
    "- SARSA\n",
    "- Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 강화학습과 정책 평가 1 : Monte-Carlo Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화학습 과정\n",
    "\n",
    "1. 일단 진행\n",
    "2. 자신 평가\n",
    "3. 평가\n",
    "4. 업데이트\n",
    "\n",
    "5. 1-4 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 강화학습의 예측과 제어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`'Dynamic Programming'`에서의 Policy Iteration 방법 중 `Policy evaluation`은  `bellman expectation equation`을 사용하여 `value function`을 계산하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **bellman expectation equation**\n",
    "> \n",
    "$$\n",
    "\\mathbf{v}_\\pi(\\mathbf{s}) = \\mathbb{E}_\\pi[\\mathbf{R_{t+1}} + \\gamma \\mathbf{v}_\\pi(\\mathbf{S}_{t+1}) \\ | \\ \\mathbf{S}_{t} = \\mathbf{s}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만, 강화학습에서는 위와 같이 **계산**을 통해서 **`value function`**을 알아내는 것이 아니라 **Agent**가 겪은 경험으로부터 예측하여 **`value function`**(참 값)을 업데이트 함, 많은 강화학습 알고리즘 중에 고전적 방법인 **Monte-Carlo Prediction**를 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte-Carlo Prediction 근사의 예시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보통 원의 넓이를 계산하기 위해 원의 방정식을 이용한다.\n",
    "\n",
    "> 원의 넓이를 구하는 방정식\n",
    "$$\n",
    "\\mathbf{S} = \\pi\\mathbf{r}^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1)** 하지만, 방정식을 모른다면 어떻게 계산할 수 있을까?\n",
    "\n",
    "**A1)** **Monte-Carlo Approximation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Monte-Carlo Approximation` : \"무작위로 무엇인가를 해본다\", \"Sample\"을 통해 \"원래의 값에 대해 이럴 것이다\"라고 추정하는 것, 무작위로 많이하다보면 원래의 값과 비슷해짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Example1)** \n",
    "종이 위에 그려진 원의 넓이를 구하려면 어떻게 해야 할까? (단, 종이의 넓이는 $\\mathbf{S(B)}$이고 $\\mathbf{S(B)}$는 이미 안다고 가정)\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/RL/blob/master/RL_Document/Ch4_1_MonteCarlo/FIGURE/FIG_1.png?raw=true\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "원이 그려진 종이 위에 점을 무작위로 계속 뿌린다.\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/RL/blob/master/RL_Document/Ch4_1_MonteCarlo/FIGURE/FIG_2.png?raw=true\" alt=\"drawing\" width=\"330\"/>\n",
    "\n",
    "$$\n",
    "\\frac{\\mathbf{S(A)}}{\\mathbf{S(B)}} \\sim \\frac{\\mathbf{1}}{\\mathbf{11}} \\sum_{i = 1}^{11} \\mathbf{I(\\text{red_dot}_i \\in \\mathbf{A})} = \\frac{\\mathbf{4}}{\\mathbf{11}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MC 추정은 무한히 반복하면 원래 값과 동일해진다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "$\n",
    "\\text{if n } \\to \\infty , \\text{then} \\\\\n",
    "\\frac{1}{n} \\sum_{i = 1}^{n} \\mathbf{I(\\text{red_dot}_i \\in \\mathbf{A})} = \\frac{\\mathbf{S(A)}}{\\mathbf{S(B)}}\n",
    "$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "장점 : 근사의 장점은 방정식을 몰라도 원래 값을 추정할 수 있는 점,\n",
    "\n",
    "강화학습에서 MC 근사를 어떻게 사용하는지 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 샘플링과 몬테카를로 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Monte-Carlo Prediction**를 통한 **Value Function** 추정의 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/DeepHaeJoong/RL/blob/master/RL_Document/Ch4_1_MonteCarlo/FIGURE/FIG_3.png?raw=true\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 기존 **state-value function** 정의\n",
    ">\n",
    "$$\n",
    "\\mathbf{v}_\\pi(\\mathbf{s}) = \\mathbb{E}_\\pi[\\mathbf{R_{t+1}} + \\gamma \\mathbf{v}_\\pi(\\mathbf{S}_{t+1}) \\ | \\ \\mathbf{S}_{t} = \\mathbf{s}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic Programming에서는 Bellman Expectation Equation을 통해 전체 문제를 작게 쪼개서 풀었지만, **`Sampling`**을 통해 기댓값을 계산하지 않고, \"예측\"하려면 어떻게 해야할까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 끝이 있는 `에피소드`에서의 반환값 정의 (한번의 샘플링)\n",
    ">\n",
    "$$\n",
    "\\mathbf{G_t} = \\mathbf{R}_{t+1} + \\gamma\\mathbf{R_{t+2}} + ... + \\gamma^{\\mathbf{T-t+1}}\\mathbf{R_{T}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한 번의 에피소드를 통해 반환값을 얻어도 한 번의 에피소드로는 추정이 불가능하다. (비유 : 원의 넓이를 구할 때 점을 한 번 찍는 것과 같음). 따라서, 여러번 에피소드를 통해 각 상태에 대해 모인 반환값들의 평균을 통해 가치함수의 값을 추정해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 계산 가능한 형태의 Bellman Expectation Equation\n",
    "> \n",
    "> \n",
    "> $\n",
    "1. \\mathbf{v}_\\pi(\\mathbf{s}) = \\mathbb{E}_\\pi[\\mathbf{R}_{t+1} + \\gamma\\mathbf{R_{t+2}} + ... \\ | \\ \\mathbf{S}_{t} = \\mathbf{s}] \\\\\n",
    "2. \\mathbf{v}_\\pi(\\mathbf{s}) = \\mathbb{E}_\\pi[\\mathbf{R_{t+1}} + \\gamma \\mathbf{v}_\\pi(\\mathbf{S}_{t+1}) \\ | \\ \\mathbf{S}_{t} = \\mathbf{s}] \\\\\n",
    "3. \\mathbf{v}_\\pi(\\mathbf{s}) = \\sum_{\\mathbf{a} \\in A} \\pi(\\mathbf{a}|\\mathbf{s}) \\ (\\mathbf{R_{t+1}} + \\gamma \\sum_{\\mathbf{s'} \\in S} \\mathbf{P_{ss'}^{\\mathbf{a}}} \\mathbf{v}_\\pi(\\mathbf{s'}))\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만, 3.에서 환경의 모델인 $\\mathbf{P_{ss'}^{\\mathbf{a}}}$ 와 $\\mathbf{R_{t+1}}$을 알아야 한다. 마치 원의 넓이를 구할 때 원의 넓이의 방정식을 알아야 하는 것과 마찬가지이다. 여기서는 환경의 모델을 몰라도 여러 에피소드를 통해 구한 반환값의 평균을 통해 $\\mathbf{v}_\\pi(\\mathbf{s})$를 추정할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\mathbf{s}$라는 상태를 방문해서 얻은 반환값들의 평균을 통해 Value Function을 추정하는 식\n",
    "> \n",
    "$$\n",
    "\\mathbf{v}_\\pi(\\mathbf{s}) \\sim \\frac{\\mathbf{1}}{\\mathbf{N(s)}} \\sum_{\\mathbf{i=1}}^{\\mathbf{N(s)}} \\mathbf{G_i(s)}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "> 에피소드를 통해 $\\mathbf{s}$를 지나 $\\mathbf{s_T}$ (마침 상태)까지의 반환값들의 평균 (그림)\n",
    "> \n",
    "<img src=\"https://github.com/DeepHaeJoong/RL/blob/master/RL_Document/Ch4_1_MonteCarlo/FIGURE/FIG_4.png?raw=true\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **업데이트식의 전개 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{V}_{\\mathbf{n+1}} = \\frac{\\mathbf{1}}{\\mathbf{n}} \\sum_{\\mathbf{i=1}}^{\\mathbf{n}} \\mathbf{G_i} = \\frac{\\mathbf{1}}{\\mathbf{n}} (\\mathbf{G_n} + \\sum_{\\mathbf{i=1}}^{\\mathbf{n-1}} \\mathbf{G_i})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **업데이트식의 전개 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{V}_{\\mathbf{n+1}} = \\frac{\\mathbf{1}}{\\mathbf{n}} (\\mathbf{G_n} + (\\mathbf{n-1})\\frac{1}{\\mathbf{n-1}}\\sum_{\\mathbf{i=1}}^{\\mathbf{n-1}} \\mathbf{G_i})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\qquad \\mathbf{V}_{\\mathbf{n}} = \\frac{1}{\\mathbf{n-1}}\\sum_{\\mathbf{i=1}}^{\\mathbf{n-1}} \\mathbf{G_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **업데이트식의 전개 3**\n",
    ">\n",
    "$$\n",
    "\\mathbf{V}_{\\mathbf{n+1}} = \\frac{\\mathbf{1}}{\\mathbf{n}}(\\mathbf{G_n} + (\\mathbf{n-1})\\mathbf{V}_{\\mathbf{n}})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{V}_{\\mathbf{n+1}} = \\frac{\\mathbf{1}}{\\mathbf{n}}(\\mathbf{G_n} + \\mathbf{n}\\mathbf{V}_{\\mathbf{n}} - \\mathbf{V}_{\\mathbf{n}})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{V}_{\\mathbf{n+1}} = \\mathbf{V}_{\\mathbf{n}} + \\frac{\\mathbf{1}}{\\mathbf{n}}(\\mathbf{G_n} - \\mathbf{V}_{\\mathbf{n}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Monte-Carlo Prediction**에서 **Value Function** 업데이트 식\n",
    ">\n",
    "$$\n",
    "\\mathbf{V(s)} \\leftarrow \\mathbf{V(s)}+ \\frac{\\mathbf{1}}{\\mathbf{n}}(\\mathbf{G(s)} - \\mathbf{V(s)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 오차 : $\\mathbf{G(s)} - \\mathbf{V(s)}$\n",
    "- Step Size : $\\frac{\\mathbf{1}}{\\mathbf{n}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Monte-Carlo Prediction**에서 **Value Function** 업데이트의 **일반식**\n",
    ">\n",
    "$$\n",
    "\\mathbf{V(s)} \\leftarrow \\mathbf{V(s)}+ \\alpha(\\mathbf{G(s)} - \\mathbf{V(s)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Monte-Carlo Prediction** 이후의 모든 강화학습 방법에서 **Value Funcation**을 업데이트하는 것은 위의 식을 변형한 것 (이해 중요)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
