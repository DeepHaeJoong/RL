{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal-difference Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화학습에서 가장 중요한 아이디어는 Temporal-difference(시간차)\n",
    "\n",
    "- `Monte-Carlo Prediction`의 단점\n",
    "\n",
    "\n",
    "    - 가치함수를 업데이트 하기 위해서는 에피소드가 끝날 때까지 기다려야 함 (실시간이 아니라는 점)\n",
    "    - 에피소드의 끝이 없거나 에피소드의 길이가 긴 경우에 예측은 정확하지 않을 것\n",
    "\n",
    "위의 `Monte-Carlo Prediction`의 단점인 에피소드마다가 아니라 **타임스텝마다** 가치함수를 업데이트하는 방법이 Temporal-difference Prediction 임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction**\n",
    "\n",
    "가치함수를 학습하는 것, 크게 두 가지 방법이 존재\n",
    "\n",
    "- MonteCarlo\n",
    "- **Temporal-difference (TD)**\n",
    "\n",
    "**Control**\n",
    "\n",
    "가치함수를 토대로 정책을 발전시켜 최적 정책을 찾는 것\n",
    "\n",
    "- SARSA\n",
    "- Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 강화학습과 정책 평가 2 : 시간차 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 시간차 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Monte-Carlo Prediction**에서 **Value Function** 업데이트의 **일반식**\n",
    ">\n",
    "$$\n",
    "\\mathbf{V(S_t)} \\leftarrow \\mathbf{V(S_t)}+ \\alpha(\\mathbf{G_t} - \\mathbf{V(S_t)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{G_t}$ 대신 $\\mathbf{R_{t+1}} + \\gamma \\mathbf{v}_\\pi(\\mathbf{S'})$을 이용\n",
    "\n",
    "**위의 값을 샘플링해서 그 값으로 현재의 Value Function을 업데이트!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Temporal-difference Prediction**에서 **Value Function** 업데이트\n",
    ">\n",
    "$$\n",
    "\\mathbf{V(S_t)} \\leftarrow \\mathbf{V(S_t)}+ \\alpha(\\mathbf{R} + \\gamma \\mathbf{V}(\\mathbf{S_{t+1}}) - \\mathbf{V(S_{t})})\n",
    "$$\n",
    "\n",
    "1. $\\mathbf{R} + \\gamma \\mathbf{V}(\\mathbf{S_{t+1}})$ = 업데이트의 목표\n",
    "2. $\\alpha(\\mathbf{R} + \\gamma \\mathbf{V}(\\mathbf{S_{t+1}}) - \\mathbf{V(S_{t})})$ = 업데이트의 크기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{R} + \\gamma \\mathbf{V}(\\mathbf{S_{t+1}}) - \\mathbf{V(S_{t})}$를 Temporal-difference Error(시간차 에러)이라고 부름\n",
    "\n",
    "시간차 예측에서 업데이트의 목표는 반환값과는 달리 실제의 값은 아니다. \n",
    "\n",
    "$\\mathbf{V}(\\mathbf{S_{t+1}})$은 현재 Agent가 가지고 있는 값이다. \n",
    "\n",
    "에이전트는 이 값을 $\\mathbf{S_{t+1}}$의 가치함수일 것이라고 예측\n",
    "\n",
    "이처럼 다른 상태의 가치함수 예측값을 통해 지금 상태의 가치함수를 예측하는 이러한 방식을 부트스트랩이라고 한다. 즉, 업데이트 목표도 정확하지 않은 상황에서 가치함수를 업데이트 하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Temporal-difference Prediction**에서 **Value Function** 업데이트 (그림)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/DeepHaeJoong/RL/blob/master/RL_Document/Ch4_2_Temporal-Difference/FIGURE/FIG_1.png?raw=true\" alt=\"drawing\" width=\"350\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
