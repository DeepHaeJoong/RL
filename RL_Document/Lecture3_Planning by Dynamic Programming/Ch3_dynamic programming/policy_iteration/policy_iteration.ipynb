{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정리\n",
    "\n",
    "## MDP\n",
    "---\n",
    "\n",
    "\n",
    "순차적 행동결정 문제를 수학적으로 정의한 것, **MDP**는 다음과 같은 항목으로 구성하고 있다.\n",
    "\n",
    "- $S$ (state) : \n",
    "- $A$ (action) :\n",
    "- $R$ (reward) :\n",
    "- $P_{ss'}^{a}$ (state transition probability) :\n",
    "- $\\gamma$ (discount factor) : \n",
    "- $\\pi(a,s)$ (policy): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value function\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent가 어떤 $\\pi$(정책)가 더 좋은 정책인지 판단하는 기준이 value function\n",
    "#### **s**(현재 상태)로부터 $\\pi$(정책)을 따라갔을 때 받을 것이라 예상되는 Reward의 합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **state-value function**\n",
    ">\n",
    "$$\n",
    "\\mathbf{v}_\\pi(\\mathbf{s}) = \\mathbb{E}_\\pi[\\mathbf{R_{t+1}} + \\gamma \\mathbf{v}_\\pi(\\mathbf{S}_{t+1}) \\ | \\ \\mathbf{S}_{t} = \\mathbf{s}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Agent는 $\\pi$(정책)을 업데이트할 때 value function를 사용할 텐데, 보통 value function보다는 value function가 선택할 각 action의 value를 직접적으로 나타내는 q function을 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **action-value function**\n",
    ">\n",
    "$$\n",
    "\\mathbf{q}_\\pi(\\mathbf{s, a}) = \\mathbb{E}_\\pi[\\mathbf{R_{t+1}} + \\gamma \\mathbf{q}_\\pi(\\mathbf{S}_{t+1}, \\mathbf{A}_{t+1}) \\ | \\ \\mathbf{S}_{t} = \\mathbf{s}, \\mathbf{A}_{t} = \\mathbf{a}]\n",
    "$$\n",
    ">\n",
    "> **action-value function** (연산가능하도록 또 다른 형태)\n",
    ">\n",
    "$$\n",
    "\\mathbf{q}_\\pi(\\mathbf{s, a}) = \\mathbf{R_{t+1}} + \\gamma \\sum_{\\mathbf{s'} \\in S} \\mathbf{P_{ss'}^{\\mathbf{a}}} \\mathbf{v}_\\pi(\\mathbf{s'})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman equation\n",
    "---\n",
    "\n",
    "> $\\mathbf{s}_t$(현재 상태)의 가치함수와 $\\mathbf{s}_{t+1}$(다음 상태)의 가치함수 관계식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Bellman Expectation Equation** : \n",
    "> 특정 정책을 따라갔을 때 **state-value function** 사이의 관계식\n",
    ">\n",
    "$$\n",
    "\\mathbf{v}_\\pi(\\mathbf{s}) = \\mathbb{E}_\\pi[\\mathbf{R_{t+1}} + \\gamma \\mathbf{v}_\\pi(\\mathbf{S}_{t+1}) \\ | \\ \\mathbf{S}_{t} = \\mathbf{s}]\n",
    "$$\n",
    "> 가치 함수와 큐 함수의 관계식 (계산 O)\n",
    "> \n",
    "$$\n",
    "\\mathbf{v}_\\pi(\\mathbf{s}) = \\sum_{\\mathbf{a} \\in A} \\pi(\\mathbf{a}|\\mathbf{s}) \\ \\mathbf{q}_\\pi(\\mathbf{s, a})\n",
    "$$\n",
    ">\n",
    "> 계산 가능한 Bellman Equation\n",
    ">\n",
    "$$\n",
    "\\mathbf{v}_\\pi(\\mathbf{s}) = \\sum_{\\mathbf{a} \\in A} \\pi(\\mathbf{a}|\\mathbf{s}) \\ (\\mathbf{R_{t+1}} + \\gamma \\sum_{\\mathbf{s'} \\in S} \\mathbf{P_{ss'}^{\\mathbf{a}}} \\mathbf{v}_\\pi(\\mathbf{s'}))\n",
    "$$\n",
    ">\n",
    "> 계산 가능한 Bellman Equation\n",
    ">\n",
    "$$\n",
    "\\mathbf{v}_\\pi(\\mathbf{s}) = \\sum_{\\mathbf{a} \\in A} \\pi(\\mathbf{a}|\\mathbf{s}) \\ (\\mathbf{R_{t+1}} + \\gamma \\sum_{\\mathbf{s'} \\in S} \\mathbf{P_{ss'}^{\\mathbf{a}}} \\mathbf{v}_\\pi(\\mathbf{s'}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Bellman Optimality Equation**\n",
    "> 최적의 가치함수를 받게하는 정책을 따라갔을 때 가치함수 사이의 관계식\n",
    ">\n",
    "\n",
    "$$\n",
    "\\mathbf{v}_*(\\mathbf{s}) = \\max_{a} \\ \\mathbb{E}[\\mathbf{R_{t+1}} + \\gamma \\mathbf{v}_*(\\mathbf{S}_{t+1}) \\ | \\ \\mathbf{S}_{t} = \\mathbf{s}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Programming\n",
    "___\n",
    "\n",
    "- **Dynamic** : sequential or temporal component to the problem\n",
    "- **Programming** : optimising a “program”, i.e. a policy\n",
    "    - A method for solving complex problems\n",
    "    - By breaking them down into subproblems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bellman Expectation Equation을 푸는 것이 **Policy Iteration** (지금 다루고자 하는 내용) \n",
    "- Bellman Optimality Equation을 푸는 것이 **Value Iteration** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Policy Iteration\n",
    "\n",
    "Bellman Expectation Equation 푸는 것 : 주변 상태의 value function과 다음 스텝의 보상만 고려하여 현재 상태의 다음 가치함수를 계산\n",
    "\n",
    "Policy Iteration는 Policy evaluation 과 Policy improvement로 구성\n",
    "\n",
    "- Policy evaluation : Estimate $\\mathbf{v}_\\pi$ Iterative policy evaluation\n",
    "- Policy improvement : Generate $\\pi^{'}$ $\\ge$ $\\pi$ Greedy policy improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/DeepHaeJoong/RL/blob/master/RL_Document/Lecture3_Planning%20by%20Dynamic%20Programming/Ch3_dynamic%20programming/policy_iteration/FIGURE/FIG_1.png?raw=true\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Policy evaluation (정책 평가)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Bellman Expectation Equation**을 통한 효율적인 **value function** 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{v}_\\pi(\\mathbf{s}) = \\mathbb{E}_\\pi[\\mathbf{R_{t+1}} + \\gamma \\mathbf{v}_\\pi(\\mathbf{S}_{t+1}) \\ | \\ \\mathbf{S}_{t} = \\mathbf{s}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 합의 형태로 표현한 **Bellman Equation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{v}_\\pi(\\mathbf{s}) = \\sum_{\\mathbf{a} \\in A} \\pi(\\mathbf{a}|\\mathbf{s}) \\ (\\mathbf{R_{t+1}} + \\gamma \\sum_{\\mathbf{s'} \\in S} \\mathbf{P_{ss'}^{\\mathbf{a}}} \\mathbf{v}_\\pi(\\mathbf{s'}))\n",
    "$$\n",
    "\n",
    "> $\\pi$라는 정책에 대해 반복적으로 수행 (k = 1, 2, 3, 4, ....)\n",
    "\n",
    "$$\n",
    "\\mathbf{v}_{k+1}(\\mathbf{s}) = \\sum_{\\mathbf{a} \\in A} \\pi(\\mathbf{a}|\\mathbf{s}) \\ (\\mathbf{R_{t+1}} + \\gamma \\sum_{\\mathbf{s'} \\in S} \\mathbf{P_{ss'}^{\\mathbf{a}}} \\mathbf{v}_{k}(\\mathbf{s'}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example :  grid world\n",
    "\n",
    "- $S$ (state) : red box\n",
    "- $A$ (action) : [up, down, left, right]\n",
    "- $R$ (reward) : Green tringle (-1), Blue circle (+1)\n",
    "- $P_{ss'}^{\\mathbf{a}}$ (state transition probability) : 1\n",
    "- $\\gamma$ (discount factor) : 0.9\n",
    "- $\\pi(\\mathbf{a} | \\mathbf{s})$ (policy): $\\frac{1}{4}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/DeepHaeJoong/RL/blob/master/RL_Document/Lecture3_Planning%20by%20Dynamic%20Programming/Ch3_dynamic%20programming/policy_iteration/FIGURE/FIG_2.png?raw=true\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 알고리즘\n",
    "\n",
    "**1.** k번째 **value function** Matrix에서 $\\mathbf{s}$에서 갈 수 있는 다음 상태 $\\mathbf{s'}$에 저장돼 있는 value function $\\mathbf{v}_k (\\mathbf{s'})$을 불러온다.\n",
    "\n",
    "\n",
    "**2.** $\\mathbf{v}_k (\\mathbf{s'})$에 $\\gamma$를 곱하고 그 상태로 가능 행동에 대한 보상($\\mathbf{R_{s}^{\\mathbf{a}}}$)을 더한다.\n",
    "\n",
    "$$\n",
    "\\mathbf{R_{s}^{\\mathbf{a}}} + \\gamma \\mathbf{v}_k (\\mathbf{s'})\n",
    "$$\n",
    "\n",
    "\n",
    "**3.** 2번에서 구한 값에 그 행동을 할 확률, 즉 정책을 곱한다.\n",
    "\n",
    "$$\n",
    "\\pi(\\mathbf{a}|\\mathbf{s})(\\mathbf{R_{s}^{\\mathbf{a}}} + \\gamma \\mathbf{v}_k (\\mathbf{s'}))\n",
    "$$\n",
    "\n",
    "\n",
    "**4.** 3번을 모든 선택 가능한 행동에 대해 반복하고 그 값들을 더한다.\n",
    "\n",
    "$$\n",
    "\\sum_{\\mathbf{a} \\in A} \\pi(\\mathbf{a}|\\mathbf{s})(\\mathbf{R_{s}^{\\mathbf{a}}} + \\gamma \\mathbf{v}_k (\\mathbf{s'}))\n",
    "$$\n",
    "\n",
    "\n",
    "**5.** 4번 과정을 통해 더한 값을 k+1 번째 가치함수 행렬에 상태 s자리에 저장한다.\n",
    "\n",
    "\n",
    "**6.** 1-5 과정이 모든 $\\mathbf{s} \\in S$에 대해 반복한다.\n",
    "\n",
    "$$\n",
    "\\mathbf{s} \\in S\n",
    "$$\n",
    "\n",
    "\n",
    "$\\pi$라는 정책에 대해 반복적으로 수행 (k = 1, 2, 3, 4, ....)하면 $\\mathbf{v}_\\pi$가 수렴될 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Policy improvement (정책 발전)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Greedy Policy Improvement** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 앞서 정책 평가를 통해 구한 것은 에이전트가 정책을 따랐을 때의 모든 상태에 대한 가치 함수(**state-value function**)이다. 어떻게 이 가치함수를 통해 각 상태에 대해 어떤 행동을 하는 것이 좋은지 알 수 있을까? 큐함수(**action-value function**)을 이용하면 어떤 행동이 좋은지 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **action-value function(큐함수)** 정의 \n",
    "> \n",
    "$$\n",
    "\\mathbf{q}_\\pi(\\mathbf{s, a}) = \\mathbb{E}_\\pi[\\mathbf{R_{t+1}} + \\gamma \\mathbf{q}_\\pi(\\mathbf{S}_{t+1}, \\mathbf{A}_{t+1}) \\ | \\ \\mathbf{S}_{t} = \\mathbf{s}, \\mathbf{A}_{t} = \\mathbf{a}]\n",
    "$$\n",
    "\n",
    "> **action-value function(큐함수)** (연산가능하도록 또 다른 형태)\n",
    ">\n",
    "$$\n",
    "\\mathbf{q}_\\pi(\\mathbf{s, a}) = \\mathbf{R_{t+1}} + \\gamma \\sum_{\\mathbf{s'} \\in S} \\mathbf{P_{ss'}^{\\mathbf{a}}} \\mathbf{v}_\\pi(\\mathbf{s'})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent가 해야할 일은 상태 $\\mathbf{s}$에서 선택 가능한 행동 $\\mathbf{q}_\\pi(\\mathbf{s, a})$를 비교하고 그중에서 가장 큰 큐함수를 가지는 행동을 선택하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Greedy Policy Improvement**으로 얻은 새로운 정책\n",
    ">\n",
    "$$\n",
    "\\pi^{'}(\\mathbf{s}) = \\mathbf{argmax}_{\\mathbf{a} \\in A} \\mathbf{q}_\\pi(\\mathbf{s, a})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example :  grid world\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/RL/blob/master/RL_Document/Lecture3_Planning%20by%20Dynamic%20Programming/Ch3_dynamic%20programming/policy_iteration/FIGURE/FIG_3.png?raw=true\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Code \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### policy_iteration.py (전체 코드)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import random\n",
    "from environment import GraphicDisplay, Env\n",
    "\n",
    "\n",
    "class PolicyIteration:\n",
    "    def __init__(self, env):\n",
    "        # 환경에 대한 객체 선언\n",
    "        self.env = env\n",
    "        # 가치함수를 2차원 리스트로 초기화\n",
    "        self.value_table = [[0.0] * env.width for _ in range(env.height)]\n",
    "        # 상 하 좌 우 동일한 확률로 정책 초기화\n",
    "        self.policy_table = [[[0.25, 0.25, 0.25, 0.25]] * env.width for _ in range(env.height)]\n",
    "        # 마침 상태의 설정\n",
    "        self.policy_table[2][2] = []\n",
    "        # 감가율\n",
    "        self.discount_factor = 0.9\n",
    "\n",
    "\n",
    "    def policy_evaluation(self):\n",
    "\n",
    "        # 다음 가치함수 초기화\n",
    "        next_value_table = [[0.00] * self.env.width for _ in range(self.env.height)]\n",
    "\n",
    "        # 모든 상태에 대해서 벨만 기대방정식을 계산\n",
    "        for state in self.env.get_all_states():\n",
    "            value = 0.0\n",
    "            # 마침 상태의 가치 함수 = 0\n",
    "            if state == [2, 2]:\n",
    "                next_value_table[state[0]][state[1]] = value\n",
    "                continue\n",
    "\n",
    "            # 벨만 기대 방정식\n",
    "            for action in self.env.possible_actions:\n",
    "                next_state = self.env.state_after_action(state, action)\n",
    "                reward = self.env.get_reward(state, action)\n",
    "                next_value = self.get_value(next_state)\n",
    "                value += (self.get_policy(state)[action] * (reward + self.discount_factor * next_value))\n",
    "\n",
    "            next_value_table[state[0]][state[1]] = round(value, 2)\n",
    "\n",
    "        self.value_table = next_value_table\n",
    "\n",
    "    # 현재 가치 함수에 대해서 탐욕 정책 발전\n",
    "    def policy_improvement(self):\n",
    "        next_policy = self.policy_table\n",
    "        for state in self.env.get_all_states():\n",
    "            if state == [2, 2]:\n",
    "                continue\n",
    "            value = -99999\n",
    "            max_index = []\n",
    "            # 반환할 정책 초기화\n",
    "            result = [0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "            # 모든 행동에 대해서 [보상 + (감가율 * 다음 상태 가치함수)] 계산\n",
    "            for index, action in enumerate(self.env.possible_actions):\n",
    "                next_state = self.env.state_after_action(state, action)\n",
    "                reward = self.env.get_reward(state, action)\n",
    "                next_value = self.get_value(next_state)\n",
    "                temp = reward + self.discount_factor * next_value\n",
    "\n",
    "                # 받을 보상이 최대인 행동의 index(최대가 복수라면 모두)를 추출\n",
    "                if temp == value:\n",
    "                    max_index.append(index)\n",
    "                elif temp > value:\n",
    "                    value = temp\n",
    "                    max_index.clear()\n",
    "                    max_index.append(index)\n",
    "\n",
    "            # 행동의 확률 계산\n",
    "            prob = 1 / len(max_index)\n",
    "\n",
    "            for index in max_index:\n",
    "                result[index] = prob\n",
    "\n",
    "            next_policy[state[0]][state[1]] = result\n",
    "\n",
    "        self.policy_table = next_policy\n",
    "\n",
    "    # 특정 상태에서 정책에 따른 행동을 반환\n",
    "    def get_action(self, state):\n",
    "        # 0 ~ 1 사이의 값을 무작위로 추출\n",
    "        random_pick = random.randrange(100) / 100\n",
    "\n",
    "        policy = self.get_policy(state)\n",
    "        policy_sum = 0.0\n",
    "        # 정책에 담긴 행동 중에 무작위로 한 행동을 추출\n",
    "        for index, value in enumerate(policy):\n",
    "            policy_sum += value\n",
    "            if random_pick < policy_sum:\n",
    "                return index\n",
    "\n",
    "            \n",
    "    # 상태에 따른 정책 반환\n",
    "    def get_policy(self, state):\n",
    "        if state == [2, 2]:\n",
    "            return 0.0\n",
    "        return self.policy_table[state[0]][state[1]]\n",
    "\n",
    "    # 가치 함수의 값을 반환\n",
    "    def get_value(self, state):\n",
    "        # 소숫점 둘째 자리까지만 계산\n",
    "        return round(self.value_table[state[0]][state[1]], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전체 코드의 흐름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIteration:\n",
    "    def __init__(self, env):\n",
    "        # 환경에 대한 객체\n",
    "        self.env = env\n",
    "        \n",
    "    # 정책 평가\n",
    "    def policy_evaluation(self):\n",
    "        pass\n",
    "    \n",
    "    # 정책 발전\n",
    "    def policy_improvement(self):\n",
    "        pass\n",
    "    \n",
    "    # 특정 상태에서 정책에 따른 행동\n",
    "    def get_action(self, state):\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 `__init__`\n",
    "\n",
    "- value_table\n",
    "- policy_table\n",
    "- discount_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "필요한 정보를 변수로 선언\n",
    "'''\n",
    "def __init__(self, env):\n",
    "    # 환경에 대한 객체 선언\n",
    "    self.env = env\n",
    "    # value_table : 가치함수를 2차원 리스트로 초기화\n",
    "    self.value_table = [[0.0] * env.width for _ in range(env.height)]\n",
    "    # value_table : 상 하 좌 우 동일한 확률로 정책 초기화 (5 x 5 x 4) 3차원 리스트\n",
    "    self.policy_table = [[[0.25, 0.25, 0.25, 0.25]] * env.width for _ in range(env.height)]\n",
    "    # 마침 상태의 설정\n",
    "    self.policy_table[2][2] = []\n",
    "    # discount_factor : 감가율\n",
    "    self.discount_factor = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 `policy_evaluation`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\mathbf{v}_{k+1}(\\mathbf{s}) = \\sum_{\\mathbf{a} \\in A} \\pi(\\mathbf{a}|\\mathbf{s}) \\ (\\mathbf{R_{t+1}} + \\gamma \\sum_{\\mathbf{s'} \\in S} \\mathbf{P_{ss'}^{\\mathbf{a}}} \\mathbf{v}_{k}(\\mathbf{s'}))\n",
    "$$\n",
    "\n",
    "$\\mathbf{P_{ss'}^{\\mathbf{a}}}$ = 1 로 가정하고 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(self):\n",
    "\n",
    "    # 다음 가치함수 초기화\n",
    "    next_value_table = [[0.00] * self.env.width for _ in range(self.env.height)]\n",
    "\n",
    "    # 모든 상태에 대해서 벨만 기대방정식을 계산\n",
    "    for state in self.env.get_all_states():\n",
    "        value = 0.0\n",
    "        # 마침 상태의 가치 함수 = 0\n",
    "        if state == [2, 2]:\n",
    "            next_value_table[state[0]][state[1]] = value\n",
    "            continue\n",
    "\n",
    "        # 벨만 기대 방정식\n",
    "        for action in self.env.possible_actions:\n",
    "            next_state = self.env.state_after_action(state, action)\n",
    "            reward = self.env.get_reward(state, action)\n",
    "            next_value = self.get_value(next_state)\n",
    "            value += (self.get_policy(state)[action] * (reward + self.discount_factor * next_value))\n",
    "\n",
    "        next_value_table[state[0]][state[1]] = round(value, 2)\n",
    "\n",
    "    self.value_table = next_value_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 `policy_improvement`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{q}_\\pi(\\mathbf{s, a}) = \\mathbf{R_{t+1}} + \\gamma \\sum_{\\mathbf{s'} \\in S} \\mathbf{P_{ss'}^{\\mathbf{a}}} \\mathbf{v}_\\pi(\\mathbf{s'})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 가치 함수에 대해서 탐욕 정책 발전\n",
    "def policy_improvement(self):\n",
    "    next_policy = self.policy_table\n",
    "    for state in self.env.get_all_states():\n",
    "        if state == [2, 2]:\n",
    "            continue\n",
    "        value = -99999\n",
    "        max_index = []\n",
    "        # 반환할 정책 초기화\n",
    "        result = [0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "        # 모든 행동에 대해서 [보상 + (감가율 * 다음 상태 가치함수)] 계산\n",
    "        for index, action in enumerate(self.env.possible_actions):\n",
    "            next_state = self.env.state_after_action(state, action)\n",
    "            reward = self.env.get_reward(state, action)\n",
    "            next_value = self.get_value(next_state)\n",
    "            temp = reward + self.discount_factor * next_value\n",
    "\n",
    "            # 받을 보상이 최대인 행동의 index(최대가 복수라면 모두)를 추출\n",
    "            if temp == value:\n",
    "                max_index.append(index)\n",
    "            elif temp > value:\n",
    "                value = temp\n",
    "                max_index.clear()\n",
    "                max_index.append(index)\n",
    "\n",
    "        # 행동의 확률 계산\n",
    "        prob = 1 / len(max_index)\n",
    "\n",
    "        for index in max_index:\n",
    "            result[index] = prob\n",
    "\n",
    "        next_policy[state[0]][state[1]] = result\n",
    "\n",
    "    self.policy_table = next_policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
