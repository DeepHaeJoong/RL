{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-Carlo Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Dynamic Programming`과 `Reinforcement Learning`의 차이 : RL은 환경의 모델을 몰라도 환경과의 상호작용을 통해 최적 정책을 학습하는 것\n",
    "\n",
    "(**Model Free**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction**\n",
    "\n",
    "가치함수를 학습하는 것, 크게 두 가지 방법이 존재\n",
    "\n",
    "- **MonteCarlo** \n",
    "- Temporal-difference (TD)\n",
    "\n",
    "**Control**\n",
    "\n",
    "가치함수를 토대로 정책을 발전시켜 최적 정책을 찾는 것\n",
    "\n",
    "- SARSA\n",
    "- Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 강화학습과 정책 평가 1 : Monte-Carlo Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화학습 과정\n",
    "\n",
    "1. 일단 진행\n",
    "2. 자신 평가\n",
    "3. 평가\n",
    "4. 업데이트\n",
    "\n",
    "5. 1-4 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 강화학습의 예측과 제어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`'Dynamic Programming'`에서의 Policy Iteration 방법 중 `Policy evaluation`은  `bellman expectation equation`을 사용하여 `value function`을 계산하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **bellman expectation equation**\n",
    "> \n",
    "$$\n",
    "\\mathbf{v}_\\pi(\\mathbf{s}) = \\mathbb{E}_\\pi[\\mathbf{R_{t+1}} + \\gamma \\mathbf{v}_\\pi(\\mathbf{S}_{t+1}) \\ | \\ \\mathbf{S}_{t} = \\mathbf{s}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만, 강화학습에서는 위와 같이 **계산**을 통해서 **`value function`**을 알아내는 것이 아니라 **Agent**가 겪은 경험으로부터 예측하여 **`value function`**(참 값)을 업데이트 함, 많은 강화학습 알고리즘 중에 고전적 방법인 **Monte-Carlo Prediction**를 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte-Carlo Prediction 근사의 예시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보통 원의 넓이를 계산하기 위해 원의 방정식을 이용한다.\n",
    "\n",
    "> 원의 넓이를 구하는 방정식\n",
    "$$\n",
    "\\mathbf{S} = \\pi\\mathbf{r}^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1)** 하지만, 방정식을 모른다면 어떻게 계산할 수 있을까?\n",
    "\n",
    "**A1)** **Monte-Carlo Approximation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Monte-Carlo Approximation` : \"무작위로 무엇인가를 해본다\", \"Sample\"을 통해 \"원래의 값에 대해 이럴 것이다\"라고 추정하는 것, 무작위로 많이하다보면 원래의 값과 비슷해짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Example1)** \n",
    "종이 위에 그려진 원의 넓이를 구하려면 어떻게 해야 할까? (단, 종이의 넓이는 $\\mathbf{S(B)}$이고 $\\mathbf{S(B)}$는 이미 안다고 가정)\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/RL/blob/master/RL_Document/Lecture4_Model-Free%20Prediction/Ch4_1_Monte-Carlo/FIGURE/FIG_1.png?raw=true\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "원이 그려진 종이 위에 점을 무작위로 계속 뿌린다.\n",
    "\n",
    "<img src=\"https://github.com/DeepHaeJoong/RL/blob/master/RL_Document/Lecture4_Model-Free%20Prediction/Ch4_1_Monte-Carlo/FIGURE/FIG_2.png?raw=true\" alt=\"drawing\" width=\"330\"/>\n",
    "\n",
    "$$\n",
    "\\frac{\\mathbf{S(A)}}{\\mathbf{S(B)}} \\sim \\frac{\\mathbf{1}}{\\mathbf{11}} \\sum_{i = 1}^{11} \\mathbf{I(\\text{red_dot}_i \\in \\mathbf{A})} = \\frac{\\mathbf{4}}{\\mathbf{11}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MC 추정은 무한히 반복하면 원래 값과 동일해진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "$\n",
    "\\text{if n } \\to \\infty , \\text{then} \\\\\n",
    "\\frac{1}{n} \\sum_{i = 1}^{n} \\mathbf{I(\\text{red_dot}_i \\in \\mathbf{A})} = \\frac{\\mathbf{S(A)}}{\\mathbf{S(B)}}\n",
    "$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "장점 : 근사의 장점은 방정식을 몰라도 원래 값을 추정할 수 있는 점,\n",
    "\n",
    "강화학습에서 MC 근사를 어떻게 사용하는지 알아보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 샘플링과 몬테카를로 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Monte-Carlo Prediction**를 통한 **Value Function** 추정의 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/DeepHaeJoong/RL/blob/master/RL_Document/Lecture4_Model-Free%20Prediction/Ch4_1_Monte-Carlo/FIGURE/FIG_3.png?raw=true\" alt=\"drawing\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 기존 **state-value function** 정의\n",
    ">\n",
    "$$\n",
    "\\mathbf{v}_\\pi(\\mathbf{s}) = \\mathbb{E}_\\pi[\\mathbf{R_{t+1}} + \\gamma \\mathbf{v}_\\pi(\\mathbf{S}_{t+1}) \\ | \\ \\mathbf{S}_{t} = \\mathbf{s}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic Programming에서는 Bellman Expectation Equation을 통해 전체 문제를 작게 쪼개서 풀었지만, **`Sampling`**을 통해 기댓값을 계산하지 않고, \"예측\"하려면 어떻게 해야할까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 끝이 있는 `에피소드`에서의 반환값 정의 (한번의 샘플링)\n",
    ">\n",
    "$$\n",
    "\\mathbf{G_t} = \\mathbf{R}_{t+1} + \\gamma\\mathbf{R_{t+2}} + ... + \\gamma^{\\mathbf{T-t+1}}\\mathbf{R_{T}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한 번의 에피소드를 통해 반환값을 얻어도 한 번의 에피소드로는 추정이 불가능하다. (비유 : 원의 넓이를 구할 때 점을 한 번 찍는 것과 같음). 따라서, 여러번 에피소드를 통해 각 상태에 대해 모인 반환값들의 평균을 통해 가치함수의 값을 추정해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 계산 가능한 형태의 Bellman Expectation Equation\n",
    ">\n",
    "$$\n",
    "1. \\mathbf{v}_\\pi(\\mathbf{s}) = \\mathbb{E}_\\pi[\\mathbf{R}_{t+1} + \\gamma\\mathbf{R_{t+2}} + ... \\ | \\ \\mathbf{S}_{t} = \\mathbf{s}]\n",
    "$$\n",
    "\n",
    "$$\n",
    "2. \\mathbf{v}_\\pi(\\mathbf{s}) = \\mathbb{E}_\\pi[\\mathbf{R_{t+1}} + \\gamma \\mathbf{v}_\\pi(\\mathbf{S}_{t+1}) \\ | \\ \\mathbf{S}_{t} = \\mathbf{s}]\n",
    "$$\n",
    "\n",
    "$$\n",
    "3. \\mathbf{v}_\\pi(\\mathbf{s}) = \\sum_{\\mathbf{a} \\in A} \\pi(\\mathbf{a}|\\mathbf{s}) \\ (\\mathbf{R_{t+1}} + \\gamma \\sum_{\\mathbf{s'} \\in S} \\mathbf{P_{ss'}^{\\mathbf{a}}} \\mathbf{v}_\\pi(\\mathbf{s'}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만, 3.에서 환경의 모델인 $\\mathbf{P_{ss'}^{\\mathbf{a}}}$ 와 $\\mathbf{R_{t+1}}$을 알아야 한다. 마치 원의 넓이를 구할 때 원의 넓이의 방정식을 알아야 하는 것과 마찬가지이다. 여기서는 환경의 모델을 몰라도 여러 에피소드를 통해 구한 반환값의 평균을 통해 $\\mathbf{v}_\\pi(\\mathbf{s})$를 추정할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $\\mathbf{s}$라는 상태를 방문해서 얻은 반환값들의 평균을 통해 Value Function을 추정하는 식\n",
    "> \n",
    "$$\n",
    "\\mathbf{v}_\\pi(\\mathbf{s}) \\sim \\frac{\\mathbf{1}}{\\mathbf{N(s)}} \\sum_{\\mathbf{i=1}}^{\\mathbf{N(s)}} \\mathbf{G_i(s)}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "> 에피소드를 통해 $\\mathbf{s}$를 지나 $\\mathbf{s_T}$ (마침 상태)까지의 반환값들의 평균 (그림)\n",
    "> \n",
    "<img src=\"https://github.com/DeepHaeJoong/RL/blob/master/RL_Document/Lecture4_Model-Free%20Prediction/Ch4_1_Monte-Carlo/FIGURE/FIG_4.png?raw=true\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **업데이트식의 전개 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{V}_{\\mathbf{n+1}} = \\frac{\\mathbf{1}}{\\mathbf{n}} \\sum_{\\mathbf{i=1}}^{\\mathbf{n}} \\mathbf{G_i} = \\frac{\\mathbf{1}}{\\mathbf{n}} (\\mathbf{G_n} + \\sum_{\\mathbf{i=1}}^{\\mathbf{n-1}} \\mathbf{G_i})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **업데이트식의 전개 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{V}_{\\mathbf{n+1}} = \\frac{\\mathbf{1}}{\\mathbf{n}} (\\mathbf{G_n} + (\\mathbf{n-1})\\frac{1}{\\mathbf{n-1}}\\sum_{\\mathbf{i=1}}^{\\mathbf{n-1}} \\mathbf{G_i})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\qquad \\mathbf{V}_{\\mathbf{n}} = \\frac{1}{\\mathbf{n-1}}\\sum_{\\mathbf{i=1}}^{\\mathbf{n-1}} \\mathbf{G_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **업데이트식의 전개 3**\n",
    ">\n",
    "$$\n",
    "\\mathbf{V}_{\\mathbf{n+1}} = \\frac{\\mathbf{1}}{\\mathbf{n}}(\\mathbf{G_n} + (\\mathbf{n-1})\\mathbf{V}_{\\mathbf{n}})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{V}_{\\mathbf{n+1}} = \\frac{\\mathbf{1}}{\\mathbf{n}}(\\mathbf{G_n} + \\mathbf{n}\\mathbf{V}_{\\mathbf{n}} - \\mathbf{V}_{\\mathbf{n}})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{V}_{\\mathbf{n+1}} = \\mathbf{V}_{\\mathbf{n}} + \\frac{\\mathbf{1}}{\\mathbf{n}}(\\mathbf{G_n} - \\mathbf{V}_{\\mathbf{n}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Monte-Carlo Prediction**에서 **Value Function** 업데이트 식\n",
    ">\n",
    "$$\n",
    "\\mathbf{V(s)} \\leftarrow \\mathbf{V(s)}+ \\frac{\\mathbf{1}}{\\mathbf{n}}(\\mathbf{G(s)} - \\mathbf{V(s)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 오차 : $\\mathbf{G(s)} - \\mathbf{V(s)}$\n",
    "- Step Size : $\\frac{\\mathbf{1}}{\\mathbf{n}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Monte-Carlo Prediction**에서 **Value Function** 업데이트의 **일반식**\n",
    ">\n",
    "$$\n",
    "\\mathbf{V(s)} \\leftarrow \\mathbf{V(s)}+ \\alpha(\\mathbf{G(s)} - \\mathbf{V(s)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Monte-Carlo Prediction** 이후의 모든 강화학습 방법에서 **Value Funcation**을 업데이트하는 것은 위의 식을 변형한 것 (이해 중요)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/DeepHaeJoong/RL/blob/master/RL_Document/Lecture4_Model-Free%20Prediction/Ch4_1_Monte-Carlo/FIGURE/FIG_5.png?raw=true\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $\\mathbf{G(s)}$ = 업데이트의 목표\n",
    "2. $\\alpha(\\mathbf{G(s)} - \\mathbf{V(s)})$ = 업데이트의 크기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Agent`**는 위 업데이트 식을 통해 에피소드 동안 경험했던 모든 상태에 대해 가치함수를 업데이트한다.\n",
    "\n",
    "어떠한 가치함수가 업데이트될 수록 가치함수는 현재 정책에 대한 참 가치함수에 수렴해 간다. \n",
    "\n",
    "다음에 다룰 **`Temporal-difference prediction`**방법에서는 업데이트의 목표($\\mathbf{G(s)}$)가 변하고 나머지는 동일!\n",
    "\n",
    "#### Things to do\n",
    "- code 확인 필요 : MonteCarlo Prediction은 Model Free이기 때문에 Pss' 존재하지 않을것으로 예상\n",
    "- code 구현 : ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from environment import Env\n",
    "\n",
    "class MCAgent:\n",
    "    def __init__(self, env):\n",
    "        # 환경에 대한 객체 선억\n",
    "        self.env = env\n",
    "        # 가치함수를 2차원 리스트로 초기화 (5 * 5)\n",
    "        self.value_table = [[0.] * env.width for _ in range(env.height)]\n",
    "        # 상 하 좌 우 동일한 확률로 Poliy 초기화\n",
    "        self.policy_table = [[[0.25, 0.25, 0.25, 0.25]] * env.width for _ in range(env.height)]\n",
    "        # 마침 상태의 설정\n",
    "        self.policy_table[2][2] = []\n",
    "        # 감가율\n",
    "        self.discount_facotr = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../img/rectangle.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-8849706c9b51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#agent = MCAgent(actions=list(range(env.n_actions)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMCIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\git\\RL\\RL_Document\\Lecture4_Model-Free Prediction\\Ch4_1_Monte-Carlo\\environment.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'monte carlo'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeometry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{0}x{1}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHEIGHT\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mUNIT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHEIGHT\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mUNIT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_canvas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\git\\RL\\RL_Document\\Lecture4_Model-Free Prediction\\Ch4_1_Monte-Carlo\\environment.py\u001b[0m in \u001b[0;36mload_images\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         rectangle = PhotoImage(\n\u001b[1;32m---> 48\u001b[1;33m             Image.open(\"../img/rectangle.png\").resize((65, 65)))\n\u001b[0m\u001b[0;32m     49\u001b[0m         triangle = PhotoImage(\n\u001b[0;32m     50\u001b[0m             Image.open(\"../img/triangle.png\").resize((65, 65)))\n",
      "\u001b[1;32m~\\study\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2764\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2765\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2766\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2767\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2768\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../img/rectangle.png'"
     ]
    }
   ],
   "source": [
    "env = Env()\n",
    "#agent = MCAgent(actions=list(range(env.n_actions)))\n",
    "agent = MCIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ALLUSERSPROFILE': 'C:\\\\ProgramData',\n",
       " 'APPDATA': 'C:\\\\Users\\\\LHJ\\\\AppData\\\\Roaming',\n",
       " 'COMMONPROGRAMFILES': 'C:\\\\Program Files\\\\Common Files',\n",
       " 'COMMONPROGRAMFILES(X86)': 'C:\\\\Program Files (x86)\\\\Common Files',\n",
       " 'COMMONPROGRAMW6432': 'C:\\\\Program Files\\\\Common Files',\n",
       " 'COMPUTERNAME': 'DESKTOP-7JL2LFO',\n",
       " 'COMSPEC': 'C:\\\\Windows\\\\system32\\\\cmd.exe',\n",
       " 'DRIVERDATA': 'C:\\\\Windows\\\\System32\\\\Drivers\\\\DriverData',\n",
       " 'FPS_BROWSER_APP_PROFILE_STRING': 'Internet Explorer',\n",
       " 'FPS_BROWSER_USER_PROFILE_STRING': 'Default',\n",
       " 'HOMEDRIVE': 'C:',\n",
       " 'HOMEPATH': '\\\\Users\\\\LHJ',\n",
       " 'LOCALAPPDATA': 'C:\\\\Users\\\\LHJ\\\\AppData\\\\Local',\n",
       " 'LOGONSERVER': '\\\\\\\\DESKTOP-7JL2LFO',\n",
       " 'NUMBER_OF_PROCESSORS': '8',\n",
       " 'ONEDRIVE': 'C:\\\\Users\\\\LHJ\\\\OneDrive',\n",
       " 'OS': 'Windows_NT',\n",
       " 'PATH': 'C:\\\\Windows\\\\system32;C:\\\\Windows;C:\\\\Windows\\\\System32\\\\Wbem;C:\\\\Windows\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\;C:\\\\Windows\\\\System32\\\\OpenSSH\\\\;C:\\\\Program Files\\\\PuTTY\\\\;C:\\\\Program Files\\\\Git\\\\cmd;C:\\\\Users\\\\LHJ\\\\study;C:\\\\Users\\\\LHJ\\\\study\\\\Library\\\\mingw-w64\\\\bin;C:\\\\Users\\\\LHJ\\\\study\\\\Library\\\\usr\\\\bin;C:\\\\Users\\\\LHJ\\\\study\\\\Library\\\\bin;C:\\\\Users\\\\LHJ\\\\study\\\\Scripts;C:\\\\Users\\\\LHJ\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps;C:\\\\Users\\\\LHJ\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps;C:\\\\Program Files\\\\JetBrains\\\\PyCharm 2019.3.3\\\\bin;',\n",
       " 'PATHEXT': '.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC',\n",
       " 'PROCESSOR_ARCHITECTURE': 'AMD64',\n",
       " 'PROCESSOR_IDENTIFIER': 'Intel64 Family 6 Model 60 Stepping 3, GenuineIntel',\n",
       " 'PROCESSOR_LEVEL': '6',\n",
       " 'PROCESSOR_REVISION': '3c03',\n",
       " 'PROGRAMDATA': 'C:\\\\ProgramData',\n",
       " 'PROGRAMFILES': 'C:\\\\Program Files',\n",
       " 'PROGRAMFILES(X86)': 'C:\\\\Program Files (x86)',\n",
       " 'PROGRAMW6432': 'C:\\\\Program Files',\n",
       " 'PROMPT': '$P$G',\n",
       " 'PSMODULEPATH': 'C:\\\\Program Files\\\\WindowsPowerShell\\\\Modules;C:\\\\Windows\\\\system32\\\\WindowsPowerShell\\\\v1.0\\\\Modules',\n",
       " 'PUBLIC': 'C:\\\\Users\\\\Public',\n",
       " 'PYCHARM': 'C:\\\\Program Files\\\\JetBrains\\\\PyCharm 2019.3.3\\\\bin;',\n",
       " 'SESSIONNAME': 'Console',\n",
       " 'SYSTEMDRIVE': 'C:',\n",
       " 'SYSTEMROOT': 'C:\\\\Windows',\n",
       " 'TEMP': 'C:\\\\Users\\\\LHJ\\\\AppData\\\\Local\\\\Temp',\n",
       " 'TMP': 'C:\\\\Users\\\\LHJ\\\\AppData\\\\Local\\\\Temp',\n",
       " 'USERDOMAIN': 'DESKTOP-7JL2LFO',\n",
       " 'USERDOMAIN_ROAMINGPROFILE': 'DESKTOP-7JL2LFO',\n",
       " 'USERNAME': 'LHJ',\n",
       " 'USERPROFILE': 'C:\\\\Users\\\\LHJ',\n",
       " 'WINDIR': 'C:\\\\Windows',\n",
       " 'KERNEL_LAUNCH_TIMEOUT': '40',\n",
       " 'JPY_INTERRUPT_EVENT': '2732',\n",
       " 'IPY_INTERRUPT_EVENT': '2732',\n",
       " 'JPY_PARENT_PID': '2728',\n",
       " 'TERM': 'xterm-color',\n",
       " 'CLICOLOR': '1',\n",
       " 'PAGER': 'cat',\n",
       " 'GIT_PAGER': 'cat',\n",
       " 'MPLBACKEND': 'module://ipykernel.pylab.backend_inline'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5)\n",
      "(5, 5, 4)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape([[0.0] * 5 for _ in range(5)]))\n",
    "print(np.shape([[[0.25, 0.25, 0.25, 0.25]] * 5 for _ in range(5)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
